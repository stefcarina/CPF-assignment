{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec39f3-a56d-4feb-8c97-ee787f4ca4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EMH said if we fully use the information we can get\n",
    "## there will be no excess return\n",
    "## price move only according to new information / surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd378def-3d3e-4ae5-b658-dc33f1197f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## weak form is simple and easy to get information like historical price and return\n",
    "## with those information cannot get excess return\n",
    "## semi-strong-form including all the public published information like news\n",
    "## with those information, the price will quickly adjust and almost impossible to get excess return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7660f-bc3f-4083-a5b4-213b19c4797d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## in efficient market, with the information, price adjust very quickly to the info.\n",
    "## therefore people cannot get returns, because the the info is priced in.\n",
    "## if you can get preditable return based on those info, that means the market is inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d192b6-759e-405d-8282-00afced625d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## isn't this a similar question as last one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ff64c7-42c2-4881-939c-2895999db196",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AR\n",
    "## can predict the future return based on the past return using the correlation between them\n",
    "## if the market is efficient. the price already reflects the information\n",
    "## means the autocorrelation should be 0\n",
    "## if autocorrelation > 0 , it has momentum\n",
    "## if autocorrelation <0, it is mean reversal\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c923a6b5-b767-41c8-886f-8a133fdf7c55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0274129-b00a-42db-8104-dcd6309d61d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"EMH testing utilities for the markets note.\n",
    "\n",
    "(c) Dr. Yves J. Hilpisch, The Python Quants GmbH.\n",
    "\n",
    "This module provides reusable functions for weak-form market efficiency tests.\n",
    "It is designed to work with a companion notebook (markets.ipynb) that handles\n",
    "data loading, visualization, and interactive exploration in the CPF Program.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "try:\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    from statsmodels.tsa.stattools import acf\n",
    "    from statsmodels.api import OLS, add_constant\n",
    "except ImportError:  # pragma: no cover\n",
    "    acorr_ljungbox = None  # type: ignore[assignment]\n",
    "    acf = None  # type: ignore[assignment]\n",
    "    OLS = None  # type: ignore[assignment]\n",
    "    add_constant = None  # type: ignore[assignment]\n",
    "\n",
    "\n",
    "def to_log_returns(prices: pd.Series) -> pd.Series:\n",
    "    \"\"\"Compute log-returns from a price series.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    prices:\n",
    "        Series of strictly positive prices indexed by date.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series of one-period log-returns aligned with the input index.\n",
    "    \"\"\"\n",
    "    prices = prices.sort_index()  # ensure increasing time index\n",
    "    ratio = prices / prices.shift(1)  # price relatives\n",
    "    log_ret = np.log(ratio)  # one-period log-returns\n",
    "    return log_ret.dropna()  # drop first NaN\n",
    "\n",
    "\n",
    "def diagnostics_summary(returns: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"Return basic diagnostics for a log-return series.\n",
    "\n",
    "    Diagnostics include mean, volatility, skewness, kurtosis, the number of\n",
    "    observations, and the number of zero returns.\n",
    "    \"\"\"\n",
    "    clean = returns.dropna()  # remove missing observations\n",
    "    stats: Dict[str, float] = {}  # container for summary statistics\n",
    "    stats[\"mean\"] = float(clean.mean())  # average log-return\n",
    "    stats[\"vol\"] = float(clean.std())  # standard deviation of returns\n",
    "    stats[\"skew\"] = float(clean.skew())  # skewness of distribution\n",
    "    stats[\"kurt\"] = float(clean.kurt())  # kurtosis of distribution\n",
    "    stats[\"n_obs\"] = float(clean.shape[0])  # number of observations\n",
    "    stats[\"n_zeros\"] = float((clean == 0.0).sum())  # zero returns\n",
    "    return stats  # return dictionary with diagnostics\n",
    "\n",
    "\n",
    "def acf_ljungbox(\n",
    "    returns: pd.Series,\n",
    "    lags: Iterable[int] = (10, 20),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute autocorrelation and Ljung–Box statistics for given lags.\n",
    "\n",
    "    The output frame contains sample autocorrelations and Ljung–Box p-values\n",
    "    for each requested lag.\n",
    "    \"\"\"\n",
    "    if acorr_ljungbox is None or acf is None:  # pragma: no cover\n",
    "        msg = \"statsmodels is required for acf_ljungbox\"\n",
    "        raise ImportError(msg)\n",
    "\n",
    "    clean = returns.dropna()  # remove missing values\n",
    "    max_lag = max(lags)  # largest lag for autocorrelation\n",
    "    acf_vals = acf(\n",
    "        clean,\n",
    "        nlags=max_lag,\n",
    "        fft=True,\n",
    "        missing=\"drop\",\n",
    "    )  # autocorrelation estimates\n",
    "\n",
    "    lb = acorr_ljungbox(\n",
    "        clean,\n",
    "        lags=list(lags),\n",
    "        return_df=True,\n",
    "    )  # Ljung–Box statistics\n",
    "\n",
    "    # build a compact summary frame\n",
    "    frame = pd.DataFrame(\n",
    "        {\n",
    "            \"lag\": list(lags),\n",
    "            \"acf\": [acf_vals[k] for k in lags],\n",
    "            \"lb_stat\": lb[\"lb_stat\"].to_numpy(),\n",
    "            \"lb_pvalue\": lb[\"lb_pvalue\"].to_numpy(),\n",
    "        }\n",
    "    ).set_index(\"lag\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def runs_test(returns: pd.Series) -> Dict[str, float]:\n",
    "    \"\"\"Perform a simple runs test on return signs.\n",
    "\n",
    "    A low p-value suggests too few or too many sign changes relative to a\n",
    "    benchmark of independent signs. This function uses a basic normal\n",
    "    approximation to keep dependencies minimal.\n",
    "    \"\"\"\n",
    "    clean = returns.dropna()  # remove missing values\n",
    "    signs = np.sign(clean.to_numpy())  # +1, 0, -1 signs\n",
    "    signs = signs[signs != 0.0]  # drop exact zeros\n",
    "\n",
    "    if signs.size == 0:\n",
    "        return {\"z_stat\": np.nan, \"p_value\": np.nan}\n",
    "\n",
    "    # count runs of consecutive identical signs\n",
    "    runs = 1  # first observation starts the first run\n",
    "    for i in range(1, signs.size):\n",
    "        if signs[i] != signs[i - 1]:\n",
    "            runs += 1  # new run starts\n",
    "\n",
    "    n_pos = int((signs > 0.0).sum())  # number of positive returns\n",
    "    n_neg = int((signs < 0.0).sum())  # number of negative returns\n",
    "\n",
    "    if n_pos == 0 or n_neg == 0:\n",
    "        return {\"z_stat\": np.nan, \"p_value\": np.nan}\n",
    "\n",
    "    # expected runs and variance under randomness\n",
    "    n = n_pos + n_neg  # total number of observations\n",
    "    mu_r = 1.0 + 2.0 * n_pos * n_neg / n  # expected number of runs\n",
    "    var_r = (\n",
    "        2.0 * n_pos * n_neg * (2.0 * n_pos * n_neg - n)\n",
    "        / (n * n * (n - 1.0))\n",
    "    )  # variance of runs\n",
    "\n",
    "    z_stat = (runs - mu_r) / np.sqrt(var_r)  # standardized test statistic\n",
    "    # two-sided p-value from normal approximation\n",
    "    from math import erf, sqrt  # import locally to avoid global state\n",
    "\n",
    "    p_value = 2.0 * (1.0 - 0.5 * (1.0 + erf(abs(z_stat) / sqrt(2.0))))\n",
    "    return {\"z_stat\": float(z_stat), \"p_value\": float(p_value)}\n",
    "\n",
    "\n",
    "def variance_ratio(\n",
    "    returns: pd.Series,\n",
    "    q_list: Iterable[int] = (2, 5, 10, 20),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Compute simple variance ratio statistics for several horizons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns:\n",
    "        Series of one-period log-returns.\n",
    "    q_list:\n",
    "        Iterable of integer horizons q for which to compute VR(q).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Frame indexed by horizon with columns ``vr`` and ``n_eff`` giving the\n",
    "        variance ratio and the effective number of non-overlapping blocks.\n",
    "    \"\"\"\n",
    "    clean = returns.dropna()  # remove missing values\n",
    "    var1 = float(clean.var(ddof=1))  # sample variance of one-period returns\n",
    "\n",
    "    records: List[Tuple[int, float, int]] = []  # container for results\n",
    "\n",
    "    for q in q_list:\n",
    "        q_int = int(q)  # ensure integer horizon\n",
    "        if q_int <= 1 or q_int > clean.shape[0]:\n",
    "            continue  # skip invalid horizons\n",
    "\n",
    "        # build non-overlapping q-period sums\n",
    "        n_block = clean.shape[0] // q_int  # number of full blocks\n",
    "        reshaped = clean.iloc[: n_block * q_int].to_numpy().reshape(\n",
    "            n_block,\n",
    "            q_int,\n",
    "        )  # blocks of length q_int\n",
    "        summed = reshaped.sum(axis=1)  # q-period returns\n",
    "        var_q = float(np.var(summed, ddof=1))  # sample variance of q-period sum\n",
    "        vr = var_q / (q_int * var1)  # variance ratio\n",
    "        records.append((q_int, vr, n_block))  # store result\n",
    "\n",
    "    frame = pd.DataFrame(\n",
    "        records,\n",
    "        columns=[\"q\", \"vr\", \"n_eff\"],\n",
    "    ).set_index(\"q\")\n",
    "    return frame\n",
    "\n",
    "\n",
    "def predictability_regression(\n",
    "    returns: pd.Series,\n",
    "    p: int=1,\n",
    "    hac_lags: int=5,\n",
    ") -> pd.Series:\n",
    "    \"\"\"Estimate an autoregression of returns with HAC-robust inference.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns:\n",
    "        Series of one-period log-returns.\n",
    "    p:\n",
    "        Lag order for the autoregression.\n",
    "    hac_lags:\n",
    "        Maximum lag for the HAC covariance estimator.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.Series\n",
    "        Series with coefficient estimates, t-statistics, and p-values.\n",
    "    \"\"\"\n",
    "    if OLS is None or add_constant is None:  # pragma: no cover\n",
    "        msg = \"statsmodels is required for predictability_regression\"\n",
    "        raise ImportError(msg)\n",
    "\n",
    "    clean = returns.dropna()  # remove missing values\n",
    "    df = pd.DataFrame({\"r\": clean})  # container for lags\n",
    "    for k in range(1, p + 1):\n",
    "        df[f\"lag_{k}\"] = df[\"r\"].shift(k)  # construct lagged returns\n",
    "    df = df.dropna()  # drop rows with incomplete lags\n",
    "\n",
    "    y = df[\"r\"]  # dependent variable\n",
    "    x = df[[f\"lag_{k}\" for k in range(1, p + 1)]]  # regressors\n",
    "    x = add_constant(x)  # add intercept\n",
    "\n",
    "    model = OLS(y, x)  # ordinary least squares model\n",
    "    results = model.fit(\n",
    "        cov_type=\"HAC\",\n",
    "        cov_kwds={\"maxlags\": hac_lags},\n",
    "    )  # HAC-robust fit\n",
    "\n",
    "    out = pd.Series(dtype=float)  # container for output\n",
    "    for name, coef in results.params.items():\n",
    "        out[f\"coef_{name}\"] = float(coef)  # coefficient estimate\n",
    "    for name, tval in results.tvalues.items():\n",
    "        out[f\"t_{name}\"] = float(tval)  # t-statistic\n",
    "    for name, pval in results.pvalues.items():\n",
    "        out[f\"p_{name}\"] = float(pval)  # p-value\n",
    "    out[\"r2\"] = float(results.rsquared)  # R-squared of regression\n",
    "    return out\n",
    "\n",
    "\n",
    "def oos_forecast_eval(\n",
    "    returns: pd.Series,\n",
    "    window: int=252,\n",
    "    costs_bps: float=2.0,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"Evaluate a simple rolling AR(1) forecast and toy trading strategy.\n",
    "\n",
    "    The strategy uses a rolling window to estimate an AR(1) model and then\n",
    "    takes positions based on the sign of the one-step-ahead forecast. Net\n",
    "    performance accounts for a symmetric round-trip cost specified in basis\n",
    "    points of notional per trade.\n",
    "    \"\"\"\n",
    "    clean = returns.dropna()  # remove missing values\n",
    "    if clean.shape[0] <= window + 1:\n",
    "        return {\n",
    "            \"mse\": np.nan,\n",
    "            \"hit_rate\": np.nan,\n",
    "            \"gross_pnl\": np.nan,\n",
    "            \"net_pnl\": np.nan,\n",
    "        }\n",
    "\n",
    "    forecasts: List[float] = []  # container for forecasts\n",
    "    realized: List[float] = []  # container for realized returns\n",
    "    positions: List[float] = []  # container for trading positions\n",
    "\n",
    "    for end in range(window, clean.shape[0] - 1):\n",
    "        sample = clean.iloc[end - window : end]  # estimation window\n",
    "        # simple AR(1) coefficient via least-squares\n",
    "        x = sample.shift(1).dropna().to_numpy()  # lagged returns\n",
    "        y = sample.loc[sample.index[1:]].to_numpy()  # aligned returns\n",
    "        if x.size == 0:\n",
    "            continue  # skip if not enough data\n",
    "        beta = float(np.dot(x, y) / np.dot(x, x))  # AR(1) slope\n",
    "        r_t = float(clean.iloc[end])  # last observed return\n",
    "        forecast = beta * r_t  # one-step-ahead forecast\n",
    "\n",
    "        r_next = float(clean.iloc[end + 1])  # realized next-period return\n",
    "        pos = float(np.sign(forecast))  # position based on forecast sign\n",
    "\n",
    "        forecasts.append(forecast)\n",
    "        realized.append(r_next)\n",
    "        positions.append(pos)\n",
    "\n",
    "    if not forecasts:\n",
    "        return {\n",
    "            \"mse\": np.nan,\n",
    "            \"hit_rate\": np.nan,\n",
    "            \"gross_pnl\": np.nan,\n",
    "            \"net_pnl\": np.nan,\n",
    "        }\n",
    "\n",
    "    f_arr = np.asarray(forecasts)  # array of forecasts\n",
    "    r_arr = np.asarray(realized)  # array of realized returns\n",
    "    p_arr = np.asarray(positions)  # array of positions\n",
    "\n",
    "    mse = float(np.mean((f_arr - r_arr) ** 2))  # mean-squared error\n",
    "    hit_rate = float(\n",
    "        np.mean(np.sign(f_arr) == np.sign(r_arr))\n",
    "    )  # frequency of correct sign\n",
    "\n",
    "    gross_ret = p_arr * r_arr  # gross strategy returns\n",
    "    turn = np.abs(np.diff(p_arr, prepend=0.0))  # position changes\n",
    "    cost_per_trade = costs_bps * 1e-4  # round-trip cost in return space\n",
    "    costs = cost_per_trade * turn  # trading costs per period\n",
    "    net_ret = gross_ret - costs  # net strategy returns\n",
    "\n",
    "    gross_pnl = float(gross_ret.sum())  # total gross PnL\n",
    "    net_pnl = float(net_ret.sum())  # total net PnL\n",
    "\n",
    "    return {\n",
    "        \"mse\": mse,\n",
    "        \"hit_rate\": hit_rate,\n",
    "        \"gross_pnl\": gross_pnl,\n",
    "        \"net_pnl\": net_pnl,\n",
    "    }\n",
    "\n",
    "\n",
    "def efficiency_scorecard(\n",
    "    returns_dict: Dict[str, pd.Series],\n",
    "    lb_lags: Iterable[int] = (10, 20),\n",
    "    vr_q: Iterable[int] = (2, 5, 10, 20),\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Build a compact efficiency scorecard for several assets.\n",
    "\n",
    "    For each asset, the scorecard includes basic distributional diagnostics,\n",
    "    short-lag autocorrelation and Ljung–Box p-values, variance ratios, and a\n",
    "    simple out-of-sample AR(1) evaluation.\n",
    "    \"\"\"\n",
    "    rows: List[pd.Series] = []  # container for per-asset summaries\n",
    "\n",
    "    for name, rets in returns_dict.items():\n",
    "        diag = diagnostics_summary(rets)  # distributional diagnostics\n",
    "        row = pd.Series(diag)  # start with diagnostics\n",
    "\n",
    "        try:\n",
    "            acf_lb = acf_ljungbox(rets, lags=lb_lags)  # autocorrelation stats\n",
    "            for lag in lb_lags:\n",
    "                row[f\"acf_{lag}\"] = float(acf_lb.loc[lag, \"acf\"])\n",
    "                row[f\"lb_p_{lag}\"] = float(acf_lb.loc[lag, \"lb_pvalue\"])\n",
    "        except Exception:  # pragma: no cover\n",
    "            pass  # keep diagnostics even if autocorrelation fails\n",
    "\n",
    "        try:\n",
    "            vr = variance_ratio(rets, q_list=vr_q)  # variance ratios\n",
    "            for q in vr.index:\n",
    "                row[f\"vr_{q}\"] = float(vr.loc[q, \"vr\"])\n",
    "        except Exception:  # pragma: no cover\n",
    "            pass  # proceed even if variance ratio computation fails\n",
    "\n",
    "        oos = oos_forecast_eval(rets)  # out-of-sample AR(1) evaluation\n",
    "        for key, value in oos.items():\n",
    "            row[f\"oos_{key}\"] = float(value)\n",
    "\n",
    "        row.name = name  # label row by asset name\n",
    "        rows.append(row)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()  # empty frame if no assets\n",
    "\n",
    "    scorecard = pd.DataFrame(rows)  # assemble frame from rows\n",
    "    return scorecard\n",
    "\n",
    "\n",
    "def plot_suite(\n",
    "    prices: pd.Series,\n",
    "    returns: pd.Series,\n",
    "    title: Optional[str]=None,\n",
    ") -> None:\n",
    "    \"\"\"Plot a basic diagnostic suite for prices and returns.\n",
    "\n",
    "    The plot includes price and log-price panels, the return series, and a\n",
    "    rolling volatility estimate based on a fixed window.\n",
    "    \"\"\"\n",
    "    log_price = np.log(prices)  # log-prices from prices\n",
    "    roll_vol = returns.rolling(window=50).std()  # rolling volatility\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(10, 6), sharex=False)\n",
    "    ax_price = axes[0, 0]\n",
    "    ax_log = axes[0, 1]\n",
    "    ax_ret = axes[1, 0]\n",
    "    ax_vol = axes[1, 1]\n",
    "\n",
    "    ax_price.plot(prices.index, prices.values, color=\"tab:blue\")\n",
    "    ax_price.set_title(\"Price\")\n",
    "\n",
    "    ax_log.plot(log_price.index, log_price.values, color=\"tab:orange\")\n",
    "    ax_log.set_title(\"Log-Price\")\n",
    "\n",
    "    ax_ret.plot(returns.index, returns.values, color=\"tab:green\")\n",
    "    ax_ret.set_title(\"Log-Returns\")\n",
    "\n",
    "    ax_vol.plot(roll_vol.index, roll_vol.values, color=\"tab:red\")\n",
    "    ax_vol.set_title(\"Rolling Volatility\")\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "\n",
    "    fig.tight_layout()  # adjust layout for readability\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2311e349-5bde-4a0e-b3f8-25460d20e911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acf</th>\n",
       "      <th>lb_stat</th>\n",
       "      <th>lb_pvalue</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lag</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.1323</td>\n",
       "      <td>44.0488</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        acf  lb_stat  lb_pvalue\n",
       "lag                            \n",
       "1   -0.1323  44.0488        0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "URL= \"https://hilpisch.com/eoddata.csv\"\n",
    "\n",
    "def load_emh_prices(symbol:str = \"SPY\") -> pd.Series:\n",
    "    df = pd.read_csv(\n",
    "        URL,\n",
    "        index_col = \"Date\",\n",
    "        parse_dates = [\"Date\"],\n",
    "    )\n",
    "    prices = df[symbol].dropna()\n",
    "    prices.name = symbol\n",
    "    return prices\n",
    "\n",
    "prices = load_emh_prices(\"SPY\")\n",
    "returns = to_log_returns(prices)\n",
    "\n",
    "acf_lb = acf_ljungbox(returns, lags=(1,))  \n",
    "acf_lb.round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ce37d-d708-4730-b2cd-ed9cd31e8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  p value = 0 means reject the null hypothsis.\n",
    "## means there is a autocorrelation. it is not consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab9ceb2a-849e-4790-a266-04b66f003664",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coef_const    0.000606\n",
       "coef_lag_1   -0.132299\n",
       "p_lag_1       0.029298\n",
       "r2            0.017506\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = predictability_regression(returns, p=1, hac_lags=5)\n",
    "reg[[\"coef_const\", \"coef_lag_1\", \"p_lag_1\", \"r2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa255ac-c0cf-41b6-8e44-3cb3569be83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we have null: beta = 0\n",
    "## p value 0.029298 < 0.05, reject.\n",
    "## so beta is not 0 and equals to -0.132299. reject efficiency\n",
    "## and this function explian 1.75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcf70f05-5d2f-4718-a9f3-a9a9742e6720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vr</th>\n",
       "      <th>n_eff</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.900618</td>\n",
       "      <td>838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.745661</td>\n",
       "      <td>502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.606471</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.694896</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          vr  n_eff\n",
       "q                  \n",
       "3   0.900618    838\n",
       "5   0.745661    502\n",
       "15  0.606471    167\n",
       "22  0.694896    114"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vr = variance_ratio(returns, q_list=(3, 5, 15, 22))\n",
    "vr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aedd680-b6fa-4704-9a7f-8000883f69cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## when days = 3, it is in line with random walk. The other three shows mean reversion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646d0a78-0ac8-4681-b41a-38fe0886808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## test if Fed rate cut  / (10 year bond yield - 2 year bond yield )has influence on euiqty index return.\n",
    "\n",
    "## set formular\n",
    "##   r(t+1) = alpha + beta * yield spread (t) + shock.\n",
    "##  null : beta = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcbbfe0b-b63c-46f2-a042-931a122a5fb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mse          0.000136\n",
       "hit_rate     0.501548\n",
       "gross_pnl   -0.323670\n",
       "net_pnl     -0.774670\n",
       "dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oos = oos_forecast_eval(returns, window=252, costs_bps=2.0)\n",
    "pd.Series(oos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f6ad20-ca83-4d81-b1cc-db685433442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## using the oos from the csv file.\n",
    "## the performance is similar to a guess based on hit_rate similar to 0.5\n",
    "## after the cose, the pnl is big.\n",
    "## so this is consistent with EMH, with information the market is efficient.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
